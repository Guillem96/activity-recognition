# Activity Recognition üèäüèº‚Äç‚ôÄÔ∏èüíÉüèº

## Obtaining candidate clips

Reproduce **Obtaining candidate clips** section from Kinetics400 paper [1].

Nowadays, accessing a set of YouTube videos is trivial, therefore to start
we can download a few videos from this source.

As stated in [1], image classifiers are simple to train using a google image
dataset with few efforts. We can take advantage of this, and approximately
detect at which time an *interesting* action occurs in the video by just 
performing inference over that video at frame level. 

To do so, we first need to create an image dataset containing images of people
doing certain actions. To get things simpler, I developed a selenium scraper 
to automatically search on google images for the desired actions and 
download the resulting images.

To generate the dataset run: 

```bash
$ python -m ar.data.build_image_ds \
    --actions data/kinetics400.names \
    --out-path data/kinetics400-image \
    --images-per-action 9999 \
    --num-workers 8
```

Once we have the dataset downloaded, we can train an image classifier using this
data by running:

```bash
$ python -m ar.models.image.train \
    --data-dir data/kinetics400-image \
    --epochs 8 \
    --batch-size 32 \
    --learning-rate 3e-5 \
    --feature-extractor densenet121 \
    --save-checkpoint model.pt
```

You can modify the hyperparameters as you wish.

Finally, we run this classifier at the frame level over a video with the 
command:

```bash
$ python -m ar.applications.generate_action_clip \
    --video-path video.mp4 \
    --image-classifier-checkpoint model.pt \
    --class-names data/kinetics400.names \
    --out-dir out/videos
```

## Literature Implementations üìî

### LRCN

Model described at [Long-term Recurrent Convolutional Networks for Visual 
Recognition and Description](https://arxiv.org/pdf/1411.4389.pdf).

![](images/ar-lrcn.png)

You can easily train it on UCF-101 using the following command:

```bash
$ python -m ar.models.video.train \
    --dataset UCF-101 \
    --data-dir data/ucf-101/videos/ --annots-dir data/ucf-101/annots/  \
    --frames-per-clip 16 --batch-size 4 --learning-rate 3e-4 \
     --save-checkpoint models/lrcn-ucf-101.pt \
     LRCN <model-args>
```

### FstCN

Model described at [Human Action Recognition using Factorized Spatio-Temporal 
Convolution Networks](https://arxiv.org/pdf/1510.00562.pdf).

![](images/ar-fstcn.png)


## Use implemented models

All the models implemented inside the `ar.video` module, are ready to use. Therefore, you can train or even finetune them using your own dataset.

```python
import ar
import torch

loss_fn = torch.nn.NLLLoss()
model = ar.video.LRCN(...)

# (BATCH, CHANNELS, FRAMES, HEIGHT, WIDTH)
video_batch = torch.randn(4, 3, 16, 224, 224)
out = model(video_batch)
loss = loss_fn(out, gt)
loss.backward()
...
```

## Reports and model weights

TBD

## Project structure

The project is thought to be a single python package called `ar`.

```
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ README.md          <- README with basic information
‚îú‚îÄ‚îÄ data/              <- Directory containing bash scripts to download the data 
|                         as well as some class to index files. 
|                         My recommendation is to store the raw data here too.
‚îÇ
‚îú‚îÄ‚îÄ notebooks          <- Jupyter notebooks. Just for exploration and examples. 
|                         By default the notebooks are not reproducible.
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt   <- `pip freeze > requirements.txt`
‚îÇ
‚îú‚îÄ‚îÄ ar                 <- Main python package. `import ar`
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ data           <- Mainly for `torch.utils.data.Dataset` and other data
|   |                     utilities.
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ models         <- Models definition and training scripts.
‚îÇ   ‚îÇ   ‚îÇ                 
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ video      <- Video models and video training scripts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ audio      <- Audio models and audio training scripts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ image      <- Image models and image training scripts
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ utils          <- Python and PyTorch utilities to avoid boilerplate code
‚îÇ   |
|   ‚îú‚îÄ‚îÄ transforms <- Data transformations compatible with `torchvision`
‚îÇ   |                 transformations API. `import ar.transforms.functional as F`
|   |                 and `import ar.transforms as T`.
|   |
|   ‚îú‚îÄ‚îÄ metrics <- Useful metrics for classification such as ``top_k_accuracy``
|   |
|   ‚îî‚îÄ‚îÄ typing.py  <- Convenient type aliases.
|
‚îî‚îÄ‚îÄ mypy.ini       <- MyPy configuration
```


## References

- [1] The Kinetics Human Action Video Dataset - https://arxiv.org/abs/1705.06950
